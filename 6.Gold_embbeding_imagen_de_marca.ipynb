{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf13e34-78ac-44c5-aa44-2a3675819866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import cuml\n",
    "from cuml.metrics.cluster.silhouette_score import cython_silhouette_score\n",
    "import random\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20b290-457f-4f34-b286-18af40d3c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### seleccionamos solamnete imagen de marca\n",
    "df_entero = pd.read_parquet(\"./data/Silver/Cleaned_data_features_productos_categorias.parquet\")\n",
    "df = df_entero[df_entero['imagen_marca']=='SI']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb41e8-0c69-41de-8cf2-259af563973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos modelo para embbeding\n",
    "model_name = 'intfloat/multilingual-e5-large-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name,\n",
    "                                torch_dtype=torch.float16,\n",
    "                                  )\n",
    "\n",
    "#quitar el batchdic to device si uso  device_map custom\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab61a8-ee8c-46c0-80d3-cc5915d82bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## funcion para sacar los embedding del modelo\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb31933-2cc6-4328-a0a2-be22a5f5794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##procesar en batch!\n",
    "\n",
    "# Crear un DataLoader para procesar los textos en lotes\n",
    "batch_size = 1024\n",
    "\n",
    "# Función para tokenizar un lote de textos\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt') #padding_side='left') qween\n",
    "\n",
    "# Convertir a DataLoader (suponiendo que tienes un DataFrame)\n",
    "dataset = df['tweets'].tolist()  # Conviértelo a lista para procesar en lotes\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Lista para guardar los embeddings\n",
    "embeddings_array = []\n",
    "\n",
    "# Asegúrate de que no calculamos gradientes\n",
    "with torch.no_grad():\n",
    "    #for batch_texts in dataloader:\n",
    "    for batch_texts in tqdm(dataloader, desc=\"Procesando batches\"):\n",
    "\n",
    "        \n",
    "        batch_dict = tokenize_batch(batch_texts)\n",
    "        batch_dict = {key: value.to(device) for key, value in batch_dict.items()} # se tiene que quiirar si device_map custom\n",
    "\n",
    "        outputs = model(**batch_dict)\n",
    "\n",
    "\n",
    "        embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "        embeddings_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "        # normalización embedding esto hace que al representar se vea igual con coseno que con distancia euclidea en UMAP\n",
    "        #mover a cpu y agregar\n",
    "        embeddings_array.append(embeddings_normalized.cpu())\n",
    "\n",
    "\n",
    "# Convertir la lista de embeddings a un solo tensor\n",
    "embedding_one_tensor = torch.cat(embeddings_array, dim=0)\n",
    "#pasar a numpy\n",
    "embeddings_numpy = embedding_one_tensor.numpy()\n",
    "\n",
    "#guardamos emmbeding umap\n",
    "np.save('./data/Silver/embeddings.npy', embeddings_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800acca-469b-4dd0-9045-4bf4c49d89c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28e4ea-e69d-4e81-9a9a-f5a91a6c2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "del embedding_one_tensor, embeddings_normalized, outputs, batch_dict, model, tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd72f6-492f-4a35-b47b-490cd6e10b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_numpy = np.load('./data/Silver/embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37b81e-96bd-4af4-845e-0dbbc7b0227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer_visual = umap.UMAP(n_components=3, random_state=2013, min_dist=0.0, metric='cosine', n_neighbors=100)  #min_dist=0.0, n_neighbors=1000,\n",
    "embedding_visual = reducer_visual.fit_transform(embeddings_numpy)\n",
    "embedding_UMAP = embedding_visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75a157-8c3f-481f-b74c-66055eebec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Busqueda de hiperparametros ## silohuete no es el mejor pero la biblioteca no tiene implementada DBCV \n",
    "\n",
    "# min_cluster_sizes = [10, 50, 100, 200, 300, 400, 500, 1000]\n",
    "# min_samples_list = [10, 50,  100, 200, 300, 400, 500, 1000]\n",
    "# cluster_selection_epsilon_list = [0.0, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "\n",
    "# results = []\n",
    "\n",
    "# count = 0\n",
    "# # Grid search\n",
    "\n",
    "# for min_cluster_size in min_cluster_sizes:\n",
    "#     for min_samples in min_samples_list:\n",
    "#       for cluster_selection_epsilon in cluster_selection_epsilon_list:\n",
    "#         #biblioteca cuml\n",
    "#         clusterer = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "#                                     min_samples=min_samples, cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "#                                     metric='euclidean')\n",
    "#         labels = clusterer.fit_predict(embedding_UMAP)\n",
    "\n",
    "#         # Ignorar si todos son ruido\n",
    "#         if len(set(labels)) <= 1 or (len(set(labels)) == 2 and -1 in labels):\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             score = cython_silhouette_score(\n",
    "#                 X=embedding_UMAP,\n",
    "#                 labels=labels,\n",
    "#                 metric='euclidean',\n",
    "#                 convert_dtype=True\n",
    "#             )\n",
    "#             results.append({\n",
    "#                 'min_cluster_size': min_cluster_size,\n",
    "#                 'min_samples': min_samples,\n",
    "#                 'silhouette': score,\n",
    "#                 'cluster_selection_epsilon':cluster_selection_epsilon,\n",
    "#                 'n_clusters': len(set(labels)) - (1 if -1 in labels else 0),\n",
    "#                 'noise_fraction': np.mean(labels == -1)\n",
    "#             })\n",
    "#             print(\"=\")\n",
    "#         except Exception as e:\n",
    "#             pass\n",
    "#         count += 1\n",
    "#         print(count)\n",
    "# # Mostrar los mejores resultados\n",
    "# results_sorted = sorted(results, key=lambda x: x['silhouette'], reverse=True)\n",
    "# for r in results_sorted[:5]:\n",
    "#     print(r)\n",
    "\n",
    "# pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b8c3a-4941-4989-ad64-3bf981e9f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#esta parete del clustering se hizo en colab y está optimizada para ello. al hacerlo con mis setting 2 devices no hay reproducibilidad para\n",
    "# la implementación de GPU de HDBSCAN.\n",
    "#por tanto copio el archivo de colab \"clusterizado_embeding_Colab.parquet\" y continuo en este cuadrerno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c386d-d82d-498b-8c97-f0f5a834f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hacemos  HDBSCAN con hiperparametros \n",
    "labels = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=400,\n",
    "                                      min_samples=400,\n",
    "                                      metric='euclidean',\n",
    "                                      cluster_selection_epsilon=0.1\n",
    "                                      ).fit_predict(embedding_UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89035575-f2d6-4b3f-9f01-8e771b91fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding = pd.concat([df.reset_index(), pd.DataFrame(embedding_visual)], axis=1)\n",
    "df_embedding.columns.values[-3:] = ['UMAP_1', 'UMAP_2', 'UMAP_3' ]\n",
    "\n",
    "df_embedding = pd.concat([df_embedding, (pd.DataFrame(labels))], axis=1)\n",
    "df_embedding.columns.values[-1:] = ['clusters_hdbscan']\n",
    "\n",
    "df_embedding['clusters_hdbscan'] = df_embedding['clusters_hdbscan'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b46c9-5436-4159-a6e6-514aebcff11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding = pd.read_parquet(\"./data/Silver/clusterizado_embeding_Colab.parquet\")\n",
    "df_backup = df_embedding.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e292c-eee4-482b-becd-20508e276c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\" \n",
    "### se observa un gran cluster que se dividirá un poco más adelante\n",
    "fig = px.scatter_3d(df_embedding,\n",
    "                    x=\"UMAP_1\",\n",
    "                    y=\"UMAP_2\",\n",
    "                    z=\"UMAP_3\",  # Tercera dimensión\n",
    "                    color='clusters_hdbscan',\n",
    "                    hover_data=['tweets', 'sentimiento'],\n",
    "                    height=1200,\n",
    "                    width=1800,\n",
    "                    color_discrete_map={-1: \"gray\"})\n",
    "\n",
    "# Actualizando las trazas para los puntos\n",
    "fig.update_traces(marker=dict(size=1, opacity=0.5))\n",
    "\n",
    "# Apagando algunos clusters desde el inicio (ej: cluster '-1')\n",
    "fig.update_traces(\n",
    "    selector=dict(name=\"-1\"),\n",
    "    marker=dict(color=\"gray\", opacity=0.1, size=1)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "#fig.write_html(\"umap_plot.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78234e-7688-45d1-a157-f53466b82eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = df_embedding.clusters_hdbscan.values\n",
    "\n",
    "#vemos de que va cada cluster \n",
    "# Iteramos sobre cada cluster, excluyendo el ruido (-1)\n",
    "for label in set(labels):\n",
    "    if label == -1:\n",
    "        continue  # Ignoramos el ruido\n",
    "    print(f\"\\nCluster {label}\")\n",
    "\n",
    "    # Accedemos a las filas del DataFrame correspondientes a cada cluster\n",
    "    cluster_tweets = df_embedding['tweets'][df_embedding['clusters_hdbscan'] == str(label)].tolist()\n",
    "\n",
    "    # Si hay más de 20 frases en el cluster, seleccionamos 20 aleatorias\n",
    "    if len(cluster_tweets) > 20:\n",
    "        cluster_tweets = random.sample(cluster_tweets, 20)  # Selecciona 20 frases aleatorias\n",
    "\n",
    "    # Imprimir las frases aleatorias\n",
    "    for tweet in cluster_tweets:\n",
    "        print(f\"- {tweet}\")  # Imprimir la frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248df3a7-17a7-4891-a1b0-f4ca2e5956f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_embedding[df_embedding['clusters_hdbscan'] == \"5\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3dd41-8448-434d-9f97-ce3a842574a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## vamos a dividir el cluster 5 \n",
    "embedding_UMAP = df[['UMAP_1','UMAP_2','UMAP_3']].values\n",
    "\n",
    "labels = cuml.cluster.hdbscan.HDBSCAN(min_cluster_size=500,\n",
    "                                      min_samples=1000,\n",
    "                                      metric='euclidean',\n",
    "                                      cluster_selection_epsilon=0.0\n",
    "                                      ).fit_predict(embedding_UMAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63656e8-d725-43d2-8e6e-1404e2823d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_embedding.columns.values[-3:] = ['UMAP_1', 'UMAP_2', 'UMAP_3' ]\n",
    "df_2 = df.copy()\n",
    "df = pd.concat([df_2.reset_index(), (pd.DataFrame(labels))], axis=1)\n",
    "df.columns.values[-1:] = ['clusters_hdbscan_2']\n",
    "\n",
    "df['clusters_hdbscan_2'] = df['clusters_hdbscan_2'].astype(str)\n",
    "df\n",
    "# df = df[['index', 'User', 'tweets', 'search', 'fecha_captura',\n",
    "#        'sentimiento', 'imagen_marca', 'sentimiento_producto',\n",
    "#        'comparativa_producto', 'producto', 'categoria', 'UMAP_1', 'UMAP_2',\n",
    "#        'UMAP_3', 'clusters_hdbscan', 'clusters_hdbscan_2']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f0e1d0-38f6-428d-bfaa-12b2f8b94095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Asumiendo que en tu DataFrame tienes la columna 'UMAP_3' para la tercera dimensión\n",
    "fig = px.scatter_3d(df,\n",
    "                    x=\"UMAP_1\",\n",
    "                    y=\"UMAP_2\",\n",
    "                    z=\"UMAP_3\",  # Tercera dimensión\n",
    "                    color='clusters_hdbscan_2',\n",
    "                    hover_data=['tweets', 'sentimiento'],\n",
    "                    height=1200,\n",
    "                    width=1800,\n",
    "                    color_discrete_map={-1: \"gray\"})\n",
    "\n",
    "# Actualizando las trazas para los puntos\n",
    "fig.update_traces(marker=dict(size=2, opacity=0.5))\n",
    "\n",
    "# Apagando algunos clusters desde el inicio (ej: cluster '-1')\n",
    "fig.update_traces(\n",
    "    selector=dict(name=\"-1\"),\n",
    "    marker=dict(color=\"gray\", opacity=0.1, size=3)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93778bc3-b31d-4d20-966b-9605f08579bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label in set(labels):\n",
    "    if label == -1:\n",
    "        continue  # Ignoramos el ruido\n",
    "    print(f\"\\nCluster {label}\")\n",
    "\n",
    "    # Accedemos a las filas del DataFrame correspondientes a cada cluster\n",
    "    cluster_tweets = df['tweets'][df['clusters_hdbscan_2'] == str(label)].tolist()\n",
    "\n",
    "    if len(cluster_tweets) > 20:\n",
    "        cluster_tweets = random.sample(cluster_tweets, 20)  # Selecciona 20 frases aleatorias\n",
    "\n",
    "    # Imprimir las frases aleatorias\n",
    "    for tweet in cluster_tweets:\n",
    "        print(f\"- {tweet}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd6679-ae55-44e5-9b85-e4d63896d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['level_0'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8f4b8d-e003-4be0-a8f1-e5ac662f55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clusters_hdbscan_2'] = df['clusters_hdbscan_2'].replace({'0': 'Humor y experiencias cotidianas', '1': 'Interacciones directas y quejas de consumidores', '2':'Críticas políticas y económicas'})\n",
    "df_embedding_merge = df[['index','clusters_hdbscan_2']]\n",
    "df_embedding_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cda502-d024-4a91-a611-f2df0fba8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(df_embedding, df_embedding_merge, on=\"index\", how='left')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39e2f8-61c3-438f-9610-a4a9e81a56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(df_embedding, df_embedding_merge, on=\"index\", how='left')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a5776-b4b0-4e39-aca0-98790d24324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corregimos valores 5\n",
    "\n",
    "for index, row in tqdm(result.iterrows(), total=len(result), desc=\"Procesando filas\"):\n",
    "    if row['clusters_hdbscan'] == '5':  # Comprobamos si el valor de la columna es '5'\n",
    "        result.at[index, 'clusters_hdbscan'] = row['clusters_hdbscan_2']  #ponemos el cluster de 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1526fd87-6dcf-405f-b0eb-e40d6d36e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##cambiamos el nombre\n",
    "\n",
    "result['clusters_hdbscan'] = result['clusters_hdbscan'].replace({\n",
    "    \"19\": \"Dulces y bollería\",\n",
    "    \"6\": \"Helados\",\n",
    "    \"12\": \"Hummus\",\n",
    "    \"4\" : \"Compra alcohol\",\n",
    "    \"11\": \"Pipas\",\n",
    "    \"13\": \"Guacamole\",\n",
    "    \"7\": \"Productos retirados\",\n",
    "    \"10\": \"Cosmética y cuidado personal \",\n",
    "    \"0\": \"Deportes\",\n",
    "    \"9\": \"Yogures y lácteos\",\n",
    "    \"3\": \"Pizzas \",\n",
    "    \"2\": \"Alcohol y bebidas alcohólicas\",\n",
    "    \"15\": \"Patatas fritas y tortillas\",\n",
    "    \"8\": \"Cereales\",\n",
    "    \"14\": \"Lasaña\",\n",
    "    \"17\": \"Pan, tartas y repostería\",\n",
    "    \"1\": \"Productos de temporada y Navidad \",\n",
    "    \"18\": \"Mochis\",\n",
    "    \"16\": \"Platos preparados, ensaladas, salsas\",\n",
    "    \"-1\": \"Desconectado\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0643cd-7641-465c-892f-cf4c7d6e5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(result,\n",
    "                    x=\"UMAP_1\",\n",
    "                    y=\"UMAP_2\",\n",
    "                    z=\"UMAP_3\",\n",
    "                    color='clusters_hdbscan',\n",
    "                    hover_data=['tweets', 'sentimiento'],\n",
    "                    height=1200,\n",
    "                    width=1800,\n",
    "                    color_discrete_map={-1: \"gray\"})\n",
    "\n",
    "# Actualizando las trazas para los puntos\n",
    "fig.update_traces(marker=dict(size=2, opacity=0.5))\n",
    "\n",
    "# Apagando algunos clusters desde el inicio (ej: cluster '-1')\n",
    "fig.update_traces(\n",
    "    selector=dict(name=\"Desconectado\"),\n",
    "    marker=dict(color=\"gray\", opacity=0.05, size=2)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964217c-e19a-4c7d-bebc-8fd8fa774916",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['fecha_captura'] = pd.to_datetime(result['fecha_captura'])\n",
    "result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b720a88-6371-468d-86f5-e2a731f673ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_parquet('./data/Gold/Gold_embedding_imagen_marca.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
